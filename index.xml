<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Keshaw Singh</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Keshaw Singh</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Keshaw Singh</copyright><lastBuildDate>Thu, 21 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Keshaw Singh</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Transfer Learning in Deep RL</title>
      <link>/project/deeprl/</link>
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/deeprl/</guid>
      <description>&lt;p&gt;The general idea of transfer learning is to reuse learnings or experiences from related tasks to obtain a &lt;em&gt;jumpstart&lt;/em&gt; on the task of interest. A lot of work in this paradigm in RL is centered around learning from &lt;em&gt;easier&lt;/em&gt;, but related tasks to generalize to desired, &lt;em&gt;harder&lt;/em&gt; task.&lt;br /&gt;
A recently published work on Challenge Learning [&lt;a href=&#34;https://drive.google.com/file/d/13lT4li8V0KKS0wqrn3Y047JiMEHCZAjF/view&#34; target=&#34;_blank&#34;&gt;Jason Ma, 2019&lt;/a&gt;] explores the other direction of transfer. It proposes learning policies on a harder version of an environment and evaluating the same on an easier one by controlling a single environment variable. The paper, however, only demonstrates experimental results on CartPole using an Advantage Actor Critic (A2C) [&lt;a href=&#34;https://arxiv.org/pdf/1602.01783.pdf&#34; target=&#34;_blank&#34;&gt;Mnih et al., 2016&lt;/a&gt;] architecture.&lt;br /&gt;
We add evaluations of the environment on an actor-critic architecture using Proximal Policy Optimization (PPO) [&lt;a href=&#34;https://arxiv.org/pdf/1707.06347.pdf&#34; target=&#34;_blank&#34;&gt;Schulman et al., 2017&lt;/a&gt;] as the actor, which is shown to be much more robust to changes in the environment. We also plan to control for other affecting variables, like mass of the cartpole, as well as present results for other control environments, like mountain car.&lt;/p&gt;

&lt;p&gt;To summarize, examine the veracity of claims made in the paper across following settings:&lt;br /&gt;
a. Choice of architecture (A2C, PPO)&lt;br /&gt;
b. Environment control variable (e.g., for CartPole - gravity, mass of cartpole)&lt;br /&gt;
c. Different OpenAI gym &lt;a href=&#34;https://gym.openai.com/envs/#classic_control&#34; target=&#34;_blank&#34;&gt;environments&lt;/a&gt; (CartPole, Mountain Car)&lt;br /&gt;
d. Evaluation strategy&lt;br /&gt;
  i. Stochastic (sample actions acc. to probability distribution)&lt;br /&gt;
  ii. Deterministic (choose action having the maximum probability)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Low-resource Neural Machine Translation</title>
      <link>/project/lorenmt/</link>
      <pubDate>Wed, 30 Oct 2019 00:00:00 +0000</pubDate>
      <guid>/project/lorenmt/</guid>
      <description>&lt;p&gt;The developments in Neural Machine Translation community (NMT) have been showing way for the rest of the NLP researchers since the last 5-6 years, especially after the advent of seq2seq models [&lt;a href=&#34;https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&#34; target=&#34;_blank&#34;&gt;Sutskever et al., 2014&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/pdf/1409.0473.pdf&#34; target=&#34;_blank&#34;&gt;Bahdanau et al., 2014&lt;/a&gt;] for translation. While more recent advances made in this field for languages like French (fr) and German (de) have been astonishing, one frequently overlooked fact is the amount of data and resources required for achieving those results.&lt;br /&gt;
Especially when we consider low-resource languages like Basque (eu) and Gujarati (gu), it would be almost impossible to replicate those lofty scores for these languages due to much less amount of freely available data. However, encouraging efforts have been made in this field to make use of transfer learning [&lt;a href=&#34;https://www.aclweb.org/anthology/D16-1163.pdf&#34; target=&#34;_blank&#34;&gt;Zoph et al., 2016&lt;/a&gt;], whereby a parent model (say, de-en) over much larger dataset is used to provide a good initial point for translation in a low-resource setting (gu-en).&lt;br /&gt;
As a part of this project, I implement an ACL 2019 paper [&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1120.pdf&#34; target=&#34;_blank&#34;&gt;Kim et al., 2019&lt;/a&gt;] transfer learning model which also uses cross-lingual maps [&lt;a href=&#34;https://arxiv.org/pdf/1710.04087.pdf&#34; target=&#34;_blank&#34;&gt;Conneau et al., 2018&lt;/a&gt;] to learn, without any need for shared vocabularies between source and target languages. Due to limitations on available resources, I am only able to replicate the general trend of results, as reported in the paper, for Basque-English and extend the framework to Gujarati-English.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Microsoft AI Challenge</title>
      <link>/project/msaic/</link>
      <pubDate>Fri, 28 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/project/msaic/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.facebook.com/msaichallenge/&#34; target=&#34;_blank&#34;&gt;Competition homepage&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Ranking passages in response to search queries on the web is a crucial task. To promote work in this area, Microsoft India organized the Microsoft AI Challenge in 2018. The task consisted of appropriately ranking ten answers for each given query and maximizing the mean reciprocal rank (MRR) over a given test set.&lt;br /&gt;
We began by working with popular models like bi-LSTM and Doc2Vec. After getting a good understanding of the data and exploring the relevant literature, we turned to using more advanced models like &lt;a href=&#34;https://arxiv.org/pdf/1707.07847.pdf&#34; target=&#34;_blank&#34;&gt;HYPERQA&lt;/a&gt; and bilateral multi-perspective matching for natural language sentences (&lt;a href=&#34;https://arxiv.org/pdf/1702.03814.pdf&#34; target=&#34;_blank&#34;&gt;BiMPM&lt;/a&gt;).&lt;br /&gt;
Through our extensive experimentation, we were able to achieve results that led us to an expected final rank under 40 (280 teams in the second stage, and 1800 overall).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Variational Inference</title>
      <link>/project/bml/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/project/bml/</guid>
      <description>&lt;p&gt;One of the central tasks in Bayesian Machine Learning is to compute the posterior distribution given data and prior. One of the popular techniques is Variational Inference. However, in its batch setting, it is often not scalable to large datasets. Fortunately, it can be done in a stochastic setting as well.&lt;br /&gt;
As a part of the project, we did a literature survey of Stochastic Variational Inference (SVI) methods. We also formulated and implemented an SVI version of Hierarchical Poisson Matrix Factorization [&lt;a href=&#34;http://jakehofman.com/inprint/poisson_recs.pdf&#34; target=&#34;_blank&#34;&gt;Gopalan et al., 2015&lt;/a&gt;], while the paper only had batch VB (Variational Bayes) updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Query-based Summarized Email Extraction</title>
      <link>/project/nlp/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/project/nlp/</guid>
      <description>&lt;p&gt;As the amount of available data around us explodes, it becomes important to be able to present the same in a concise manner. One possible use case could be searching through one&amp;rsquo;s emails for getting a brief summary of threads related to some search terms.&lt;br /&gt;
Through this project, we build a model which returns extractive summaries of emails given a search query. We start by identifying threads which have terms matching with those in the query. This step is followed by clustering the relevant emails, and finally generating brief outputs containing the most significant information for each cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fine-grained Vehicle Classification</title>
      <link>/project/ml/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/ml/</guid>
      <description>&lt;p&gt;Through this project, we build a model which learns to detect and classify moving objects in videos. A classifier learns to distinguish between pedestrians and vehicles, as well as predicting fine-grained results for vehicles like 2-wheelers, 4-wheelers, etc.&lt;br /&gt;
First, the classifier learns to distinguish between different object classes. We employ Histogram of Gradients (HOG) [&lt;a href=&#34;https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&#34; target=&#34;_blank&#34;&gt;N. Dalal and B. Triggs&lt;/a&gt;] and Scale-Invariant Feature Transform (SIFT) [&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;David G. Lowe&lt;/a&gt;] for building image features. Subsequently, we use the learned classifier with a sliding window approach over video frames to localize and identify objects.&lt;br /&gt;
An issue which we encountered while running our model over video frames was the prediction of many false positives (a window containing no object identified as having one). To address this, we turned to the approach of &lt;a href=&#34;https://www.reddit.com/r/computervision/comments/2ggc5l/what_is_hard_negative_mining_and_how_is_it/&#34; target=&#34;_blank&#34;&gt;hard negative mining&lt;/a&gt;. Here, we save a wrongly classified empty window as background and force the classifier to learn it as a separate class. This led to a significant reduction in the false positive rate.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>/home/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>/home/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
