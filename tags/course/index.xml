<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course | Keshaw Singh</title>
    <link>/tags/course/</link>
      <atom:link href="/tags/course/index.xml" rel="self" type="application/rss+xml" />
    <description>Course</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2019 Keshaw Singh</copyright><lastBuildDate>Thu, 21 Nov 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/icon-192.png</url>
      <title>Course</title>
      <link>/tags/course/</link>
    </image>
    
    <item>
      <title>Transfer Learning in Deep RL</title>
      <link>/project/deeprl/</link>
      <pubDate>Thu, 21 Nov 2019 00:00:00 +0000</pubDate>
      <guid>/project/deeprl/</guid>
      <description>&lt;p&gt;The general idea of transfer learning is to reuse learnings or experiences from related tasks to obtain a &lt;em&gt;jumpstart&lt;/em&gt; on the task of interest. A lot of work in this paradigm in RL is centered around learning from &lt;em&gt;easier&lt;/em&gt;, but related tasks to generalize to desired, &lt;em&gt;harder&lt;/em&gt; task.&lt;br /&gt;
A recently published work on Challenge Learning [&lt;a href=&#34;https://drive.google.com/file/d/13lT4li8V0KKS0wqrn3Y047JiMEHCZAjF/view&#34; target=&#34;_blank&#34;&gt;Jason Ma, 2019&lt;/a&gt;] explores the other direction of transfer. It proposes learning policies on a harder version of an environment and evaluating the same on an easier one by controlling a single environment variable. The paper, however, only demonstrates experimental results on CartPole using an Advantage Actor Critic (A2C) [&lt;a href=&#34;https://arxiv.org/pdf/1602.01783.pdf&#34; target=&#34;_blank&#34;&gt;Mnih et al., 2016&lt;/a&gt;] architecture.&lt;br /&gt;
We add evaluations of the environment on an actor-critic architecture using Proximal Policy Optimization (PPO) [&lt;a href=&#34;https://arxiv.org/pdf/1707.06347.pdf&#34; target=&#34;_blank&#34;&gt;Schulman et al., 2017&lt;/a&gt;] as the actor, which is shown to be much more robust to changes in the environment. We also plan to control for other affecting variables, like mass of the cartpole, as well as present results for other control environments, like mountain car.&lt;/p&gt;

&lt;p&gt;To summarize, examine the veracity of claims made in the paper across following settings:&lt;br /&gt;
a. Choice of architecture (A2C, PPO)&lt;br /&gt;
b. Environment control variable (e.g., for CartPole - gravity, mass of cartpole)&lt;br /&gt;
c. Different OpenAI gym &lt;a href=&#34;https://gym.openai.com/envs/#classic_control&#34; target=&#34;_blank&#34;&gt;environments&lt;/a&gt; (CartPole, Mountain Car)&lt;br /&gt;
d. Evaluation strategy&lt;br /&gt;
  i. Stochastic (sample actions acc. to probability distribution)&lt;br /&gt;
  ii. Deterministic (choose action having the maximum probability)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Variational Inference</title>
      <link>/project/bml/</link>
      <pubDate>Thu, 20 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/project/bml/</guid>
      <description>&lt;p&gt;One of the central tasks in Bayesian Machine Learning is to compute the posterior distribution given data and prior. One of the popular techniques is Variational Inference. However, in its batch setting, it is often not scalable to large datasets. Fortunately, it can be done in a stochastic setting as well.&lt;br /&gt;
As a part of the project, we did a literature survey of Stochastic Variational Inference (SVI) methods. We also formulated and implemented an SVI version of Hierarchical Poisson Matrix Factorization [&lt;a href=&#34;http://jakehofman.com/inprint/poisson_recs.pdf&#34; target=&#34;_blank&#34;&gt;Gopalan et al., 2015&lt;/a&gt;], while the paper only had batch VB (Variational Bayes) updates.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Query-based Summarized Email Extraction</title>
      <link>/project/nlp/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/project/nlp/</guid>
      <description>&lt;p&gt;As the amount of available data around us explodes, it becomes important to be able to present the same in a concise manner. One possible use case could be searching through one&amp;rsquo;s emails for getting a brief summary of threads related to some search terms.&lt;br /&gt;
Through this project, we build a model which returns extractive summaries of emails given a search query. We start by identifying threads which have terms matching with those in the query. This step is followed by clustering the relevant emails, and finally generating brief outputs containing the most significant information for each cluster.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fine-grained Vehicle Classification</title>
      <link>/project/ml/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/project/ml/</guid>
      <description>&lt;p&gt;Through this project, we build a model which learns to detect and classify moving objects in videos. A classifier learns to distinguish between pedestrians and vehicles, as well as predicting fine-grained results for vehicles like 2-wheelers, 4-wheelers, etc.&lt;br /&gt;
First, the classifier learns to distinguish between different object classes. We employ Histogram of Gradients (HOG) [&lt;a href=&#34;https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf&#34; target=&#34;_blank&#34;&gt;N. Dalal and B. Triggs&lt;/a&gt;] and Scale-Invariant Feature Transform (SIFT) [&lt;a href=&#34;https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf&#34; target=&#34;_blank&#34;&gt;David G. Lowe&lt;/a&gt;] for building image features. Subsequently, we use the learned classifier with a sliding window approach over video frames to localize and identify objects.&lt;br /&gt;
An issue which we encountered while running our model over video frames was the prediction of many false positives (a window containing no object identified as having one). To address this, we turned to the approach of &lt;a href=&#34;https://www.reddit.com/r/computervision/comments/2ggc5l/what_is_hard_negative_mining_and_how_is_it/&#34; target=&#34;_blank&#34;&gt;hard negative mining&lt;/a&gt;. Here, we save a wrongly classified empty window as background and force the classifier to learn it as a separate class. This led to a significant reduction in the false positive rate.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
