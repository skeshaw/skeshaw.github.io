[{"authors":["admin"],"categories":null,"content":"I am currently working in the algorithms team of Adobe Advertising Cloud Search (prev. Adobe Media Optimizer), where I build models which manage ad spends for businesses. I graduated from IIT Kanpur in the summer of 2017, with a major in Mathematics \u0026amp; Scientific Computing and a minor in Artificial Intelligence.\nI am fascinated by techniques that can help generate information in human languages, to accomplish useful tasks like machine translation and abstractive summarization. In particular, I am intrigued by methods based on the transfer learning paradigm, as I believe they can make up for the paucity of training data in low-resource languages.\nI am an avid football fan and spend most of my weekends watching matches. Sometimes though, I try to brush up my Spanish.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working in the algorithms team of Adobe Advertising Cloud Search (prev. Adobe Media Optimizer), where I build models which manage ad spends for businesses. I graduated from IIT Kanpur in the summer of 2017, with a major in Mathematics \u0026amp; Scientific Computing and a minor in Artificial Intelligence.\nI am fascinated by techniques that can help generate information in human languages, to accomplish useful tasks like machine translation and abstractive summarization.","tags":null,"title":"Keshaw Singh","type":"authors"},{"authors":null,"categories":null,"content":"The general idea of transfer learning is to reuse learnings or experiences from related tasks to obtain a jumpstart on the task of interest. A lot of work in this paradigm in RL is centered around learning from easier but related tasks to generalize to a desired, harder task.\nA recently published work on Challenge Learning [Jason Ma, 2019] explores the other direction of transfer. It proposes learning policies on a harder version of an environment and evaluating the same on an easier one by controlling a single environment variable. The paper, however, only demonstrates experimental results on CartPole using an Advantage Actor Critic (A2C) [Mnih et al., 2016] architecture.\nWe added evaluations of the environment on an actor-critic architecture using Proximal Policy Optimization (PPO) [Schulman et al., 2017] as the actor, which is known to be much more robust to changes in the environment. We also controlled for an extra variable, the magnitude of the horizontal force applied on the cart (a constant).\n","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"6e1166daffd546c36bf8a362512bdbd8","permalink":"/project/deeprl/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/project/deeprl/","section":"project","summary":"Evaluate using policies learned from harder tasks.","tags":["Course"],"title":"Transfer Learning in Deep RL","type":"project"},{"authors":null,"categories":null,"content":"The developments in Neural Machine Translation community (NMT) have been showing the way for the rest of the researchers in the text generation community since the last 5-6 years, especially after the advent of seq2seq models [Sutskever et al., 2014, Bahdanau et al., 2014] for translation. While more recent advances made in this field for languages like French (fr) and German (de) have been astonishing, one frequently overlooked fact is the amount of data and resources required for achieving those results.\nEspecially when we consider low-resource languages like Basque (eu) and Gujarati (gu), it would be almost impossible to replicate those lofty scores for these languages due to much less amount of freely available data. However, encouraging efforts have been made in this field to make use of transfer learning [Zoph et al., 2016], whereby a parent model (say, de-en) over much larger dataset is used to provide a good initial point for translation in a low-resource setting (gu-en).\nAs a part of this project, I implemented an ACL 2019 paper [Kim et al., 2019] on transfer learning in NMT which also uses cross-lingual maps [Conneau et al., 2018] to learn, without any need for shared vocabularies between source and target languages. Due to limitations on available resources, I was only able to replicate the general trend of results, as reported in the paper, for Basque-English and extend the framework to Gujarati-English.\n","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572393600,"objectID":"43751423d219ee2b3e451590d5c1deb7","permalink":"/project/lorenmt/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/project/lorenmt/","section":"project","summary":"NMT with transfer learning and MUSE.","tags":["Personal"],"title":"Low-resource Neural Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"Competition homepage\nRanking passages in response to search queries on the web is a crucial task. To promote work in this area, Microsoft India organized the Microsoft AI Challenge in 2018. The task consisted of appropriately ranking ten answers for each given query and maximizing the mean reciprocal rank (MRR) over a given test set.\nWe began by working with popular models like bi-LSTM and Doc2Vec. After getting a good understanding of the data and exploring the relevant literature, we turned to using more advanced models like HYPERQA and bilateral multi-perspective matching for natural language sentences (BiMPM).\nThrough our extensive experimentation, we were able to achieve results that led us to an expected final rank under 40 (out of 280 teams in the second stage and 1800 overall).\n","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"98e0618d91aa9603d6f377ccaa7fa82c","permalink":"/project/msaic/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/project/msaic/","section":"project","summary":"Answer selection given a search query.","tags":["Personal"],"title":"Microsoft AI Challenge","type":"project"},{"authors":null,"categories":null,"content":"One of the central tasks in Bayesian Machine Learning is to compute the posterior distribution given data and prior. One of the popular techniques is Variational Inference. However, in its batch setting, it is often not scalable to large datasets. Fortunately, it can be done in a stochastic setting as well.\nAs a part of the project, we did a literature survey of Stochastic Variational Inference (SVI) methods. We also formulated and implemented an SVI version of Hierarchical Poisson Matrix Factorization [Gopalan et al., 2015], while the paper only had batch VB (Variational Bayes) updates.\n","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"022742b6cef4a31efbec01039494de72","permalink":"/project/bml/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/project/bml/","section":"project","summary":"Poisson Matrix Factorization with stochastic updates.","tags":["Course"],"title":"Stochastic Variational Inference","type":"project"},{"authors":null,"categories":null,"content":"As the amount of available data around us explodes, it becomes important to be able to present the same in a concise manner. One possible use case could be searching through one\u0026rsquo;s emails for getting a brief summary of threads related to some search terms.\nThrough this project, we built a model which returned extractive summaries of emails given a search query. We started by identifying threads which had terms matching with those in the query. This step was followed by clustering the relevant emails, and finally generating brief outputs containing the most significant information for each cluster.\n","date":1479600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479600000,"objectID":"1331628aa1410d6f994721adcb8ac315","permalink":"/project/nlp/","publishdate":"2016-11-20T00:00:00Z","relpermalink":"/project/nlp/","section":"project","summary":"Extractive summaries over Enron email dataset.","tags":["Course"],"title":"Query-based Summarized Email Extraction","type":"project"},{"authors":null,"categories":null,"content":"Through this project, we built a model which learned to detect and classify moving objects in videos. A classifier learned to distinguish between pedestrians and vehicles, as well as predicting fine-grained results for vehicles like 2-wheelers, 4-wheelers, etc.\nFirst, the classifier learned to distinguish between different object classes. We employed Histogram of Gradients (HOG) [N. Dalal and B. Triggs] and Scale-Invariant Feature Transform (SIFT) [David G. Lowe] for building image features. Subsequently, we used the learned classifier with a sliding window approach over video frames to localize and identify objects.\nAn issue which we encountered while running our model over video frames was the prediction of many false positives (a window containing no object identified as having one). To address this, we turned to the approach of hard negative mining. Here, we saved a wrongly classified empty window as background and forced the classifier to learn it as a separate class. This led to a significant reduction in the false positive rate.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9626c23939f024ab3de17952aa8aebac","permalink":"/project/ml/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/ml/","section":"project","summary":"Real time object classification in video.","tags":["Course"],"title":"Fine-grained Vehicle Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]