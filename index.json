[{"authors":["admin"],"categories":null,"content":"I am currently working in the algorithms team of Adobe AdCloud Search (prev. Adobe Media Optimizer), where I build models which manage ad spends for businesses. I graduated from IIT Kanpur in the summer of 2017, with a major in Mathematics and Scientific Computing and a minor in Artificial Intelligence.\nMy research interests include neural text generation tasks, which include neural machine translation (NMT) and abstractive summarization. I have worked on a variety of projects in machine learning, including summarization, object detection, and Bayesian matrix factorization. I have also worked on transfer learning, both in machine translation and in reinforcement learning.\nFootball occupies my weekends, and I also like to play FIFA from time to time.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am currently working in the algorithms team of Adobe AdCloud Search (prev. Adobe Media Optimizer), where I build models which manage ad spends for businesses. I graduated from IIT Kanpur in the summer of 2017, with a major in Mathematics and Scientific Computing and a minor in Artificial Intelligence.\nMy research interests include neural text generation tasks, which include neural machine translation (NMT) and abstractive summarization. I have worked on a variety of projects in machine learning, including summarization, object detection, and Bayesian matrix factorization.","tags":null,"title":"Keshaw Singh","type":"authors"},{"authors":null,"categories":null,"content":"The general idea of transfer learning is to reuse learnings or experiences from related tasks to obtain a jumpstart on the task of interest. A lot of work in this paradigm in RL is centered around learning from easier, but related tasks to generalize to desired, harder task.\nA recently published work on Challenge Learning [Jason Ma, 2019] explores the other direction of transfer. It proposes learning policies on a harder version of an environment and evaluating the same on an easier one by controlling a single environment variable. The paper, however, only demonstrates experimental results on CartPole using an Advantage Actor Critic (A2C) [Mnih et al., 2016] architecture.\nWe add evaluations of the environment on an actor-critic architecture using Proximal Policy Optimization (PPO) [Schulman et al., 2017] as the actor, which is shown to be much more robust to changes in the environment. We also plan to control for other affecting variables, like mass of the cartpole, as well as present results for other control environments, like mountain car.\nTo summarize, examine the veracity of claims made in the paper across following settings:\na. Choice of architecture (A2C, PPO)\nb. Environment control variable (e.g., for CartPole - gravity, mass of cartpole)\nc. Different OpenAI gym environments (CartPole, Mountain Car)\nd. Evaluation strategy\ni. Stochastic (sample actions acc. to probability distribution)\nii. Deterministic (choose action having the maximum probability)\n","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"6e1166daffd546c36bf8a362512bdbd8","permalink":"/project/deeprl/","publishdate":"2019-11-21T00:00:00Z","relpermalink":"/project/deeprl/","section":"project","summary":"Evaluate using policies learned from harder tasks.","tags":["Course"],"title":"Transfer Learning in Deep RL","type":"project"},{"authors":null,"categories":null,"content":"The developments in Neural Machine Translation community (NMT) have been showing way for the rest of the NLP researchers since the last 5-6 years, especially after the advent of seq2seq models [Sutskever et al., 2014, Bahdanau et al., 2014] for translation. While more recent advances made in this field for languages like French (fr) and German (de) have been astonishing, one frequently overlooked fact is the amount of data and resources required for achieving those results.\nEspecially when we consider low-resource languages like Basque (eu) and Gujarati (gu), it would be almost impossible to replicate those lofty scores for these languages due to much less amount of freely available data. However, encouraging efforts have been made in this field to make use of transfer learning [Zoph et al., 2016], whereby a parent model (say, de-en) over much larger dataset is used to provide a good initial point for translation in a low-resource setting (gu-en).\nAs a part of this project, I implement an ACL 2019 paper [Kim et al., 2019] transfer learning model which also uses cross-lingual maps [Conneau et al., 2018] to learn, without any need for shared vocabularies between source and target languages. Due to limitations on available resources, I am only able to replicate the general trend of results, as reported in the paper, for Basque-English and extend the framework to Gujarati-English.\n","date":1572393600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572393600,"objectID":"43751423d219ee2b3e451590d5c1deb7","permalink":"/project/lorenmt/","publishdate":"2019-10-30T00:00:00Z","relpermalink":"/project/lorenmt/","section":"project","summary":"NMT with transfer learning and MUSE.","tags":["Personal"],"title":"Low-resource Neural Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"Competition homepage\nRanking passages in response to search queries on the web is a crucial task. To promote work in this area, Microsoft India organized the Microsoft AI Challenge in 2018. The task consisted of appropriately ranking ten answers for each given query and maximizing the mean reciprocal rank (MRR) over a given test set.\nWe began by working with popular models like bi-LSTM and Doc2Vec. After getting a good understanding of the data and exploring the relevant literature, we turned to using more advanced models like HYPERQA and bilateral multi-perspective matching for natural language sentences (BiMPM).\nThrough our extensive experimentation, we were able to achieve results that led us to an expected final rank under 40 (280 teams in the second stage, and 1800 overall).\n","date":1545955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545955200,"objectID":"98e0618d91aa9603d6f377ccaa7fa82c","permalink":"/project/msaic/","publishdate":"2018-12-28T00:00:00Z","relpermalink":"/project/msaic/","section":"project","summary":"Answer selection given a search query.","tags":["Personal"],"title":"Microsoft AI Challenge","type":"project"},{"authors":null,"categories":null,"content":"One of the central tasks in Bayesian Machine Learning is to compute the posterior distribution given data and prior. One of the popular techniques is Variational Inference. However, in its batch setting, it is often not scalable to large datasets. Fortunately, it can be done in a stochastic setting as well.\nAs a part of the project, we did a literature survey of Stochastic Variational Inference (SVI) methods. We also formulated and implemented an SVI version of Hierarchical Poisson Matrix Factorization [Gopalan et al., 2015], while the paper only had batch VB (Variational Bayes) updates.\n","date":1492646400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492646400,"objectID":"022742b6cef4a31efbec01039494de72","permalink":"/project/bml/","publishdate":"2017-04-20T00:00:00Z","relpermalink":"/project/bml/","section":"project","summary":"Poisson Matrix Factorization with stochastic updates.","tags":["Course"],"title":"Stochastic Variational Inference","type":"project"},{"authors":null,"categories":null,"content":"As the amount of available data around us explodes, it becomes important to be able to present the same in a concise manner. One possible use case could be searching through one\u0026rsquo;s emails for getting a brief summary of threads related to some search terms.\nThrough this project, we build a model which returns extractive summaries of emails given a search query. We start by identifying threads which have terms matching with those in the query. This step is followed by clustering the relevant emails, and finally generating brief outputs containing the most significant information for each cluster.\n","date":1479600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1479600000,"objectID":"1331628aa1410d6f994721adcb8ac315","permalink":"/project/nlp/","publishdate":"2016-11-20T00:00:00Z","relpermalink":"/project/nlp/","section":"project","summary":"Extractive summaries over Enron email dataset.","tags":["Course"],"title":"Query-based Summarized Email Extraction","type":"project"},{"authors":null,"categories":null,"content":"Through this project, we build a model which learns to detect and classify moving objects in videos. A classifier learns to distinguish between pedestrians and vehicles, as well as predicting fine-grained results for vehicles like 2-wheelers, 4-wheelers, etc.\nFirst, the classifier learns to distinguish between different object classes. We employ Histogram of Gradients (HOG) [N. Dalal and B. Triggs] and Scale-Invariant Feature Transform (SIFT) [David G. Lowe] for building image features. Subsequently, we use the learned classifier with a sliding window approach over video frames to localize and identify objects.\nAn issue which we encountered while running our model over video frames was the prediction of many false positives (a window containing no object identified as having one). To address this, we turned to the approach of hard negative mining. Here, we save a wrongly classified empty window as background and force the classifier to learn it as a separate class. This led to a significant reduction in the false positive rate.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9626c23939f024ab3de17952aa8aebac","permalink":"/project/ml/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/ml/","section":"project","summary":"Real time object classification in video.","tags":["Course"],"title":"Fine-grained Vehicle Classification","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8e7bc052bdfc6746ea2bb6595e8093eb","permalink":"/home/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/home/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]